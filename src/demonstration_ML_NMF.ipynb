{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "054470aa",
   "metadata": {},
   "source": [
    "## this is a demonstration of topic modelling. \n",
    "## from page 212 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "013a79d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal_newlines\n"
     ]
    }
   ],
   "source": [
    "#from foundational_file_pdf_operations\n",
    "\n",
    "import PyPDF2\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "folder = 'C:/Users/ericf/OneDrive/Documents/analytic-projects/red cross/rc_downloaded_materials/evaluations/'\n",
    "file = 'IFRC Ukraine crisis response_CVA review report_ final.pdf'\n",
    "file = 'Afghanistan_Humanitarian_Crises_MTR report_final.pdf' #very few newlines\n",
    "#file = 'Lessons Learned CVTL COVID-19 Ops-1.pdf'\n",
    "\n",
    "filepath = f'{folder}{file}'\n",
    "\n",
    "def read_document(filepath, begin_page=0, end_page=-1) -> str:\n",
    "    pdfFileObj = open(filepath, 'rb')\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "    end_page = end_page if end_page > 0 else pdfReader.numPages\n",
    "    \n",
    "    \n",
    "    page = ''\n",
    "    for i in range (begin_page, end_page):\n",
    "        pageObj = pdfReader.getPage(i)\n",
    "        page += ' ' + pageObj.extractText()\n",
    "        \n",
    "    return page\n",
    "\n",
    "document = read_document(filepath, end_page=36)\n",
    "\n",
    "def split_to_paragraphs(text, limit=100):\n",
    "    \n",
    "    \n",
    "    #clean up errant situations where two strings are merged without a space\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1. \\2', text)\n",
    "    \n",
    "    \n",
    "    # determine if newlines are used solely in the separation of paragraphs\n",
    "    # or as is common with pdf, also used to format line-lengths\n",
    "\n",
    "    chars_per_newline = len(text) / len(text.split('\\n')) \n",
    "    if chars_per_newline < limit:\n",
    "        print('excess_newlines')\n",
    "        #split the doc on sentence terminators followed by newline\n",
    "        paragraphs = re.split('[.?!]\\s*\\n', text)\n",
    "    else:\n",
    "        print('normal_newlines')\n",
    "        paragraphs = re.split(\"\\s{2,}\", text)\n",
    "        \n",
    "    #strip out any remaining newlines\n",
    "    for i in range(len(paragraphs)):\n",
    "        paragraphs[i] = paragraphs[i].replace('\\n',' ')\n",
    "\n",
    "    #create a dataframe with each paragraph as its own record\n",
    "    df_paragraphs = pd.DataFrame([{\"file\" : file, \"paragraph\" : paragraph}\n",
    "                             for paragraph in paragraphs if paragraph])\n",
    "    \n",
    "    return df_paragraphs\n",
    "    \n",
    "df_paragraph = split_to_paragraphs(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a114927",
   "metadata": {},
   "source": [
    "# End Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0c84a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e06d6fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericf\\anaconda3\\envs\\nlp_conda_test\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(221, 414)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_para_vectorizer = TfidfVectorizer(stop_words=list(stopwords), min_df=5, max_df=0.7)\n",
    "tfidf_para_vectors = tfidf_para_vectorizer.fit_transform(df_paragraph['paragraph'])\n",
    "tfidf_para_vectors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c85a09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericf\\anaconda3\\envs\\nlp_conda_test\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_text_model = NMF(n_components=10, random_state=42)\n",
    "\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_para_vectors)\n",
    "H_text_matrix = nmf_text_model.components_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd1da7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, word_vector in enumerate(model.components_):\n",
    "        total = word_vector.sum()\n",
    "        largest = word_vector.argsort()[::-1]\n",
    "        print(f\"\\nTopic {topic}\")\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\" %s (%2.2f)\" % (features[largest[i]],\n",
    "                                  word_vector[largest[i]]*100.0/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "207f264d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0\n",
      " aid (7.82)\n",
      " recipients (6.61)\n",
      " needs (3.12)\n",
      " assistance (2.83)\n",
      " kits (2.75)\n",
      "\n",
      "Topic 1\n",
      " ifrc (5.70)\n",
      " arcs (4.23)\n",
      " coordination (3.25)\n",
      " meetings (2.39)\n",
      " collaboration (2.34)\n",
      "\n",
      "Topic 2\n",
      " 2022 (11.32)\n",
      " afghanistan (9.03)\n",
      " 2021 (3.57)\n",
      " humanitarian (3.50)\n",
      " november (3.30)\n",
      "\n",
      "Topic 3\n",
      " kabul (5.98)\n",
      " district (5.27)\n",
      " nuristan (3.24)\n",
      " field (3.10)\n",
      " kandahar (3.07)\n",
      "\n",
      "Topic 4\n",
      " review (8.80)\n",
      " question (5.61)\n",
      " implementation (5.34)\n",
      " term (3.70)\n",
      " longer (3.26)\n",
      "\n",
      "Topic 5\n",
      " figure (16.60)\n",
      " recipient (10.13)\n",
      " list (8.00)\n",
      " province (7.15)\n",
      " mtr (6.79)\n",
      "\n",
      "Topic 6\n",
      " feedback (9.80)\n",
      " submit (7.33)\n",
      " complaints (6.00)\n",
      " complaint (5.92)\n",
      " mechanisms (3.74)\n",
      "\n",
      "Topic 7\n",
      " volunteers (5.42)\n",
      " arcs (3.46)\n",
      " community (2.81)\n",
      " staff (2.30)\n",
      " surveys (2.15)\n",
      "\n",
      "Topic 8\n",
      " badghis (10.65)\n",
      " male (9.90)\n",
      " female (5.93)\n",
      " 27 (3.30)\n",
      " households (3.17)\n",
      "\n",
      "Topic 9\n",
      " food (2.70)\n",
      " assistance (2.11)\n",
      " affected (2.08)\n",
      " cash (1.93)\n",
      " provinces (1.71)\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_text_model, tfidf_para_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf8b7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
